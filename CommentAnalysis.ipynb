{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75dce55e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "47b185d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 3)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаю уже обработанный текст твитов без стоп-слов\n",
    "raw_data = pd.read_csv(\"ru-tweeter-sentiment.csv\")\n",
    "raw_data = raw_data.sample(20000)\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "50801364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19964, 2)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = raw_data.drop(\"Unnamed: 0\", axis=1)\n",
    "raw_data = raw_data.dropna()\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb32bfee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text_clear', 'label'], dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62e8cfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clear</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94952</th>\n",
       "      <td>спасибо</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126503</th>\n",
       "      <td>досмотрел игру престолов зарекался смотреть не...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150183</th>\n",
       "      <td>сгоняйте мной сити молл братцы</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136222</th>\n",
       "      <td>вчерашний день вдали цивилизации немного расст...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63103</th>\n",
       "      <td>приду пирог кушать ок</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text_clear  label\n",
       "94952                                             спасибо      1\n",
       "126503  досмотрел игру престолов зарекался смотреть не...      0\n",
       "150183                     сгоняйте мной сити молл братцы      0\n",
       "136222  вчерашний день вдали цивилизации немного расст...      0\n",
       "63103                               приду пирог кушать ок      1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4d943666",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "637d3c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    'positive': 1,\n",
    "    'negative': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7a785a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "#         self.labels = [labels[label] for label in df['category']]\n",
    "        self.labels = df['label'].to_list()\n",
    "#         print(type(df['context'][0]))\n",
    "#         a = [tokenizer(text, padding = 'max_length', max_length = 512, truncation = True, return_tensors = \"pt\") for text in df['context']]\n",
    "#         self.texts = [tokenizer(text, padding = 'max_length', max_length = 512, truncation = True, return_tensors = \"pt\") for text in df['context']]\n",
    "#         self.texts = df['context'].to_list()\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text_clear']]\n",
    "#         text_tensors = []\n",
    "        \n",
    "#         for i in range(len(df)):\n",
    "#             text_tensors.append(tokenizer(raw_data['context'][i],\n",
    "#                    padding='max_length', max_length = 512, truncation=True,\n",
    "#                    return_tensors=\"pt\"))\n",
    "        \n",
    "#         self.texts = text_tensors\n",
    "    \n",
    "    \n",
    "    \n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def get_batch_labels(self, idx):\n",
    "        return np.array(self.labels[idx])\n",
    "    \n",
    "    def get_batch_texts(self, idx):\n",
    "        return self.texts[idx]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        \n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fa22d1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15971 1996 1997\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "\n",
    "df_train, df_val, df_test = np.split(raw_data.sample(frac=1, random_state=42),\n",
    "                                    [int(.8*len(raw_data)), int(.9*len(raw_data))])\n",
    "\n",
    "print(len(df_train), len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0d8b2e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dd = pd.Series(df_train.label).value_counts()\n",
    "sns.barplot(x=np.array(['negative','positive']),y=dd.values)\n",
    "plt.show()   # количество позитивных и негативных практически равно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "08d2fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout=0.5):\n",
    "        \n",
    "        super(BertClassifier, self).__init__()\n",
    "        \n",
    "        self.bert = BertModel\n",
    ".from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input_id, mask):\n",
    "        \n",
    "        _, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "        \n",
    "        return final_layerBertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd9af420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        \n",
    "    for epoch_num in range(epochs):\n",
    "        total_accuracy_train = 0\n",
    "        total_loss_train = 0\n",
    "        \n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            train_label = train_label.to(device)\n",
    "            \n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "            \n",
    "            output = model(input_id, mask)\n",
    "            \n",
    "            batch_loss = criterion(output, train_label.long())\n",
    "            total_loss_train += batch_loss.item()\n",
    "            \n",
    "            accuracy = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_accuracy_train += accuracy\n",
    "            \n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        total_accuracy_val = 0\n",
    "        total_loss_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_input, val_label in val_dataloader:\n",
    "                \n",
    "                val_label = val_label.to(device)\n",
    "\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                \n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, val_label.long())\n",
    "                total_loss_val += batch_loss.item()\n",
    "                \n",
    "                accuracy = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_accuracy_val += accuracy\n",
    "                \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_accuracy_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_accuracy_val / len(val_data): .3f}')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7f96a2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "model = BertClassifier()\n",
    "LR = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e335e941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                      | 30/7986 [01:55<8:29:52,  3.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jq/5vq5hvn94_9d1kn8sr14dbb40000gn/T/ipykernel_5898/904278755.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/jq/5vq5hvn94_9d1kn8sr14dbb40000gn/T/ipykernel_5898/1685460743.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, val_data, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "80566c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_class(text):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "    tokenized_text = tokenizer(text, \n",
    "                               padding='max_length', \n",
    "                               max_length = 512, \n",
    "                               truncation=True,\n",
    "                               return_tensors=\"pt\")\n",
    "    mask = tokenized_text['attention_mask']\n",
    "    input_id = tokenized_text['input_ids'].squeeze(1)\n",
    "\n",
    "    output = model(input_id, mask)\n",
    "    print(output)\n",
    "    return output.argmax(dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b08d8811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=5, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertClassifier()\n",
    "model.load_state_dict(torch.load(\"model_3_epochs\", map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f750ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.6960, 6.1004, 0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_class(\"Как же они прикольно на английском базарят. Акшкли ай кен тп ботом то килл анд ретёрн\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "441f598f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jq/5vq5hvn94_9d1kn8sr14dbb40000gn/T/ipykernel_3315/25811786.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Выводим облако слов на экран\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Отключаем отображение осей\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b72ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
